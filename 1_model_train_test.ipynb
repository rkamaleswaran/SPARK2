{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8357a6-f067-43ee-89bc-aa52d5912013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap\n",
    "import numpy as np, os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a8ce2-9a95-4a64-a469-1338c41dab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../real_time_sepsis_development/real_time_data/2021_6hr_preprocessed_48_0426.csv\")\n",
    "df[\"SepsisLabel\"] = df[\"SepsisLabel\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a592c598-0b0f-49fc-9331-213cb716a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sepsis patients: 2791\n",
      "Number of non-sepsis patients: 27910\n"
     ]
    }
   ],
   "source": [
    "id_sepsis = np.load(\"../real_time_sepsis_development/real_time_data/sepsis_48.npy\")\n",
    "id_nosepsis = np.load(\"../real_time_sepsis_development/real_time_data/control_resampled_48.npy\")\n",
    "\n",
    "print(\"Number of sepsis patients: {}\".format(len(id_sepsis)))\n",
    "print(\"Number of non-sepsis patients: {}\".format(len(id_nosepsis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef84169-216d-4de4-880e-f882cbe5bb35",
   "metadata": {},
   "source": [
    "# make sure to create \"xgb_model\" folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def0098-1e59-4ff5-92b4-3a26a2da35af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17d3cada-dfac-4f45-a065-52e65b13b87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample(x):\n",
    "    \n",
    "    pos = x[x[\"SepsisLabel\"] == 1]\n",
    "    neg = x[x[\"SepsisLabel\"] == 0]\n",
    "    \n",
    "    if len(pos) < len(neg):\n",
    "        neg = neg.sample(n=len(pos), replace = False, random_state = 10002)\n",
    "        \n",
    "    new = pos.append(neg)\n",
    "    new = new.sample(frac = 1, replace = False)\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "389739bb-e534-455f-8764-ecf39cb1aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BO_TPE(X_train, y_train, X_val, y_val):\n",
    "    \"Hyperparameter optimization\"\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    val = xgb.DMatrix(X_val, label=y_val)\n",
    "    X_val_D = xgb.DMatrix(X_val)\n",
    "\n",
    "    def objective(params):\n",
    "        xgb_model = xgb.train(params, dtrain=train, num_boost_round=1000, evals=[(val, 'eval')],\n",
    "                              verbose_eval=False, early_stopping_rounds=80)\n",
    "        y_vd_pred = xgb_model.predict(X_val_D, ntree_limit=xgb_model.best_ntree_limit)\n",
    "        y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "\n",
    "        acc = accuracy_score(y_val, y_val_class)\n",
    "        loss = 1 - acc\n",
    "\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    max_depths = [3, 4]\n",
    "    learning_rates = [0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 0.2]\n",
    "    subsamples = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    colsample_bytrees = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    reg_alphas = [0.0, 0.005, 0.01, 0.05, 0.1]\n",
    "    reg_lambdas = [0.8, 1, 1.5, 2, 4]\n",
    "\n",
    "    space = {\n",
    "        'max_depth': hp.choice('max_depth', max_depths),\n",
    "        'learning_rate': hp.choice('learning_rate', learning_rates),\n",
    "        'subsample': hp.choice('subsample', subsamples),\n",
    "        'colsample_bytree': hp.choice('colsample_bytree', colsample_bytrees),\n",
    "        'reg_alpha': hp.choice('reg_alpha', reg_alphas),\n",
    "        'reg_lambda': hp.choice('reg_lambda', reg_lambdas),\n",
    "    }\n",
    "\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "\n",
    "    best_param = {'max_depth': max_depths[(best['max_depth'])],\n",
    "                  'learning_rate': learning_rates[(best['learning_rate'])],\n",
    "                  'subsample': subsamples[(best['subsample'])],\n",
    "                  'colsample_bytree': colsample_bytrees[(best['colsample_bytree'])],\n",
    "                  'reg_alpha': reg_alphas[(best['reg_alpha'])],\n",
    "                  'reg_lambda': reg_lambdas[(best['reg_lambda'])]\n",
    "                  }\n",
    "\n",
    "    return best_param\n",
    "\n",
    "def train_model(k, X_train, y_train, X_val, y_val, save_model_dir):\n",
    "  \n",
    "    print('*************************************************************')\n",
    "    print('{}th training ..............'.format(k + 1))\n",
    "    print('Hyperparameters optimization')\n",
    "    best_param = BO_TPE(X_train, y_train, X_val, y_val)\n",
    "    print(\"obtained best_param\")\n",
    "    xgb_model = xgb.XGBClassifier(max_depth = best_param['max_depth'],\n",
    "                                  eta = best_param['learning_rate'],\n",
    "                                  n_estimators = 1000,\n",
    "                                  subsample = best_param['subsample'],\n",
    "                                  colsample_bytree = best_param['colsample_bytree'],\n",
    "                                  reg_alpha = best_param['reg_alpha'],\n",
    "                                  reg_lambda = best_param['reg_lambda'],\n",
    "                                  objective = \"binary:logistic\"\n",
    "                                  )\n",
    "\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='error',\n",
    "                  early_stopping_rounds=80, verbose=False)\n",
    "\n",
    "    y_tr_pred = (xgb_model.predict_proba(X_train, ntree_limit=xgb_model.best_ntree_limit))[:, 1]\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_tr_pred)\n",
    "    print('training dataset AUC: ' + str(train_auc))\n",
    "    y_tr_class = [0 if i <= 0.5 else 1 for i in y_tr_pred]\n",
    "    acc = accuracy_score(y_train, y_tr_class)\n",
    "    print('training dataset acc: ' + str(acc))\n",
    "\n",
    "    y_vd_pred = (xgb_model.predict_proba(X_val, ntree_limit=xgb_model.best_ntree_limit))[:, 1]\n",
    "\n",
    "    valid_auc = roc_auc_score(y_val, y_vd_pred)\n",
    "    print('validation dataset AUC: ' + str(valid_auc))\n",
    "    y_val_class = [0 if i <= 0.5 else 1 for i in y_vd_pred]\n",
    "    acc = accuracy_score(y_val, y_val_class)\n",
    "    print('validation dataset acc: ' + str(acc))\n",
    "    print('************************************************************')\n",
    "    # save the model\n",
    "    \n",
    "    np.save(\"y_train\" + str(k)+\".npy\", y_train)\n",
    "    np.save(\"y_train_pred\" + str(k)+\".npy\", y_tr_pred)\n",
    "    np.save(\"y_val\" + str(k)+\".npy\", y_val)\n",
    "    np.save(\"y_val_pred\" + str(k)+\".npy\", y_vd_pred)\n",
    "    \n",
    "    save_model_path = save_model_dir + 'model{}.mdl'.format(k + 1)\n",
    "    xgb_model.get_booster().save_model(fname=save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74885842-4ba0-4bf6-b96c-a596e60227fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sets = []\n",
    "val_sets = []\n",
    "ks = []\n",
    "for (k, (train0_index, val0_index)), (k, (train1_index, val1_index)) in zip(enumerate(kfold.split(train_nosepsis)), enumerate(kfold.split(train_sepsis))):\n",
    "    train_sets.append(np.append(train_nosepsis[train0_index], train_sepsis[train1_index]))\n",
    "    val_sets.append(np.append(train_nosepsis[val0_index], train_sepsis[val1_index]))\n",
    "    ks.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee1ad09-14b0-4baa-8e78-a45211126023",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"trainsets_ids.npy\", train_sets)\n",
    "np.save(\"valsets_ids.npy\", val_sets)\n",
    "np.save(\"ks.npy\", ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a3ada21-54ca-44ee-b0b3-1b87a7af9fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(37582, 167)\n",
      "(37582,)\n",
      "18791\n",
      "*************************************************************\n",
      "1th training ..............\n",
      "Hyperparameters optimization\n",
      "100%|██████████| 20/20 [41:29<00:00, 124.49s/trial, best loss: 0.2105724939679754] \n",
      "obtained best_param\n",
      "training dataset AUC: 0.9188443045106887\n",
      "training dataset acc: 0.8350540152200522\n",
      "validation dataset AUC: 0.8597879109899351\n",
      "validation dataset acc: 0.7849309059004168\n",
      "************************************************************\n",
      "1\n",
      "(36864, 167)\n",
      "(36864,)\n",
      "18432\n",
      "*************************************************************\n",
      "2th training ..............\n",
      "Hyperparameters optimization\n",
      "100%|██████████| 20/20 [40:18<00:00, 120.93s/trial, best loss: 0.24491663277755182]\n",
      "obtained best_param\n",
      "training dataset AUC: 0.8980609620058977\n",
      "training dataset acc: 0.8106011284722222\n",
      "validation dataset AUC: 0.8456392234020614\n",
      "validation dataset acc: 0.7544733631557544\n",
      "************************************************************\n",
      "2\n",
      "(37848, 167)\n",
      "(37848,)\n",
      "18924\n",
      "*************************************************************\n",
      "3th training ..............\n",
      "Hyperparameters optimization\n",
      "100%|██████████| 20/20 [33:58<00:00, 101.93s/trial, best loss: 0.23136014460009036]\n",
      "obtained best_param\n",
      "training dataset AUC: 0.9025075636010875\n",
      "training dataset acc: 0.8162122172902135\n",
      "validation dataset AUC: 0.8513853050484863\n",
      "validation dataset acc: 0.7707862629914144\n",
      "************************************************************\n",
      "3\n",
      "(37220, 167)\n",
      "(37220,)\n",
      "18610\n",
      "*************************************************************\n",
      "4th training ..............\n",
      "Hyperparameters optimization\n",
      "100%|██████████| 20/20 [36:18<00:00, 108.93s/trial, best loss: 0.23607594936708864]\n",
      "obtained best_param\n",
      "training dataset AUC: 0.9320473066169725\n",
      "training dataset acc: 0.8528747984954326\n",
      "validation dataset AUC: 0.8524218207552208\n",
      "validation dataset acc: 0.7671940928270042\n",
      "************************************************************\n",
      "4\n",
      "(37286, 167)\n",
      "(37286,)\n",
      "18643\n",
      "*************************************************************\n",
      "5th training ..............\n",
      "Hyperparameters optimization\n",
      "100%|██████████| 20/20 [30:54<00:00, 92.70s/trial, best loss: 0.23996175908221795] \n",
      "obtained best_param\n",
      "training dataset AUC: 0.9203502040296765\n",
      "training dataset acc: 0.8386793970927425\n",
      "validation dataset AUC: 0.8494173254204792\n",
      "validation dataset acc: 0.76662417675802\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "for k in ks:\n",
    "    print(k)\n",
    "    train_set = train_sets[k]\n",
    "    case = df[df[\"csn\"].isin(train_set)].reset_index(drop= True)\n",
    "    #case = case.groupby(by = [\"Unit1\", \"Unit2\"]).apply(lambda x: downsample(x)).reset_index(drop = True)\n",
    "    case = downsample(case)\n",
    "    \n",
    "    x_train = case.drop([\"csn\", \"pat_id\", \"LOS\", \"rel_time\", \"SepsisLabel\"], axis = 1).values\n",
    "    y_train = case[\"SepsisLabel\"].values\n",
    "\n",
    "    print(np.shape(x_train))\n",
    "    print(np.shape(y_train))\n",
    "    print(sum(y_train))\n",
    "    \n",
    "    \n",
    "    val_set = val_sets[k]\n",
    "    case = df[df[\"csn\"].isin(val_set)].reset_index(drop= True)\n",
    "    #case = case.groupby(by = [\"Unit1\", \"Unit2\"]).apply(lambda x: downsample(x)).reset_index(drop = True)\n",
    "    case = downsample(case)\n",
    "    \n",
    "    x_val = case.drop([\"csn\", \"pat_id\", \"LOS\", \"rel_time\", \"SepsisLabel\"], axis = 1).values\n",
    "    y_val = case[\"SepsisLabel\"].values\n",
    "\n",
    "    train_model(k, x_train, y_train, x_val, y_val, save_model_dir = './xgb_model/')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac6c30-4427-445b-b419-e0c85ec61811",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04d5c316-8435-4f60-b67e-062b93618892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_predict(X_test, k_fold, path):\n",
    "    \"ensemble the five XGBoost models by averaging their output probabilities\"\n",
    "    test_pred = np.zeros((X_test.shape[0], k_fold))\n",
    "    X_test = xgb.DMatrix(X_test)\n",
    "    for k in range(k_fold):\n",
    "        model_path_name = path + 'model{}.mdl'.format(k+1)\n",
    "        xgb_model = xgb.Booster(model_file = model_path_name)\n",
    "        y_test_pred = xgb_model.predict(X_test)\n",
    "        test_pred[:, k] = y_test_pred\n",
    "    test_pred = pd.DataFrame(test_pred)\n",
    "    result_pro = test_pred.mean(axis=1)\n",
    "\n",
    "    return result_pro\n",
    "\n",
    "\n",
    "def predict(data_set,\n",
    "            data_dir,\n",
    "            model_path,\n",
    "            risk_threshold\n",
    "            ):\n",
    "   \n",
    "    result = pd.DataFrame()\n",
    "    print(len(data_set))\n",
    "    count = 0\n",
    "    \n",
    "    for p in data_set:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        \n",
    "        patient_df = data_dir[data_dir[\"csn\"] == p]\n",
    "        \n",
    "        features = patient_df.drop([\"csn\", \"pat_id\", \"LOS\", \"rel_time\", \"SepsisLabel\"], axis = 1).values\n",
    "        \n",
    "        labels = patient_df[\"SepsisLabel\"].values\n",
    "        \n",
    "        predict_pro = load_model_predict(features, k_fold = 5, path = './' + model_path + '/')\n",
    "        \n",
    "        PredictedProbability = np.array(predict_pro)\n",
    "        PredictedLabel = [0 if i <= risk_threshold else 1 for i in predict_pro]\n",
    "        \n",
    "        temp_result = patient_df.copy()\n",
    "        temp_result = temp_result[[\"csn\", \"pat_id\", \"LOS\", \"rel_time\", \"SepsisLabel\"]]\n",
    "        temp_result[\"PredictedProbability\"] = PredictedProbability\n",
    "        temp_result[\"PredictedSepsisLabel\"] = PredictedLabel\n",
    "        result = result.append(temp_result)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef854f3-db5a-4653-80fc-fc0142394906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4606\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test_set = np.load('./real_time_data/test_set.npy')\n",
    "test_data_path = df[df[\"csn\"].isin(test_set)]\n",
    "model_path = \"xgb_model\"\n",
    "\n",
    "result = predict(test_set, test_data_path, model_path, 0.48)\n",
    "result.to_csv(\"./xgb_model/prediction_results_0426.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90901824-6c6f-43fc-a791-9dd0d761396f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
